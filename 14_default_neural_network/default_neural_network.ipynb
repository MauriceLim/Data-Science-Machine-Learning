{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network with the `default` Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complete excercise #7 from Section 10.10 of *Introduction to Statistical Learning 2e* (pg. 459)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remember to set random seeds so your work is reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Reload the dataset in case the prior modification was not intended for this task\n",
    "file_path = '/Users/yuanhanlim/Desktop/DS & ML/14_default_neural_network/default.csv'\n",
    "default_data = pd.read_csv(file_path)\n",
    "\n",
    "# Encode categorical variables\n",
    "default_data['default'] = default_data['default'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "default_data['student'] = default_data['student'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "\n",
    "# Features and target variable\n",
    "X = default_data[['income', 'balance', 'student']]\n",
    "y = default_data['default']\n",
    "\n",
    "# Standardize the features for neural network\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and test sets (70% train, 30% test)\n",
    "np.random.seed(42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Compare the neural network's performance with logistic regression\n",
    "# Logistic regression model\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(X_train, y_train)\n",
    "y_pred_logistic = logistic_model.predict(X_test)\n",
    "\n",
    "# Classification performance \n",
    "logistic_report = classification_report(y_test, y_pred_logistic, target_names=['No Default', 'Default'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No Default       0.98      1.00      0.99      2906\n",
      "     Default       0.71      0.27      0.39        94\n",
      "\n",
      "    accuracy                           0.97      3000\n",
      "   macro avg       0.85      0.63      0.69      3000\n",
      "weighted avg       0.97      0.97      0.97      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print Logistic Regression Classification Report\n",
    "print(\"\\nLogistic Regression Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred_logistic, target_names=['No Default', 'Default']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-26 13:23:06.725436: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/yuanhanlim/anaconda3/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Neural Network with Dropout Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No Default       0.97      1.00      0.99      2906\n",
      "     Default       0.76      0.14      0.23        94\n",
      "\n",
      "    accuracy                           0.97      3000\n",
      "   macro avg       0.87      0.57      0.61      3000\n",
      "weighted avg       0.97      0.97      0.96      3000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9716666666666667"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the neural network architecture with dropout\n",
    "model_with_dropout = Sequential([\n",
    "    Dense(10, activation='relu', input_shape=(X_train.shape[1],)),  # Hidden layer with 10 units\n",
    "    Dropout(0.5),  # Dropout layer with 50% rate\n",
    "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_with_dropout.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model_with_dropout.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_nn_dropout = (model_with_dropout.predict(X_test) > 0.5).astype(int).flatten()\n",
    "\n",
    "# Evaluate the accuracy of the model with dropout\n",
    "accuracy_with_dropout = accuracy_score(y_test, y_pred_nn_dropout)\n",
    "\n",
    "# Print the classification report for the model with dropout\n",
    "print(\"Neural Network with Dropout Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred_nn_dropout, target_names=['No Default', 'Default']))\n",
    "\n",
    "# Output the accuracy\n",
    "accuracy_with_dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression model slightly outperforms the neural network with dropout in identifying \"Default\" cases, achieving a higher F1-score (0.39 vs. 0.23) and recall (0.27 vs. 0.14). Both models achieve similar overall accuracy (97%) due to the imbalanced dataset dominated by \"No Default\" cases. The neural network with dropout shows potential for better precision in identifying defaults (0.76 vs. 0.71), but its recall performance limits its effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
