{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d13d9c39-4f1b-425a-8516-dbd9800a63dc",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on Hugging Face Financial Phrasebank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7e1340-87a6-40e6-934d-f307ff01f8e2",
   "metadata": {},
   "source": [
    "In this homework you will perform sentiment analysis on the *Financial Phrasebank* data set from Hugging Face.  In particular, you will use the Multinomial Naive Bayes model as well as another model of your choosing.  This is an individual assignment and your deliverable should be one or more Jupyter notebooks.\n",
    "\n",
    "The Financial Phrase bank data set consists of 4,846 financial headlines that have been labeled with the following sentiments: `0` - negative; `1` - neutral; `2` - positive.  More information about the data set can be found here: https://huggingface.co/datasets/financial_phrasebank.\n",
    "\n",
    "1. (5 pts) Devise a non-machine learning baseline against which you can assess whether your models have any predictive power.\n",
    "\n",
    "2. (15 pts) Use the `CountVectorizer` preprocessor along with `MultinomialNB` to perform sentiment analysis on the data set.  Address the following:\n",
    "   \n",
    "    a. How will you measure out-of-sample performance of the models?\n",
    "   \n",
    "    b. Compared to the baseline, does the model seem to have predictive power?\n",
    "   \n",
    "    c. Experiment with the following parameters of the model: `ngram_range`, `stop_words`, `binary`.\n",
    "   \n",
    "    d. Do any of the above parameters affect the model's performance?\n",
    "  \n",
    "3. (15 pts) Use the `TfidVectorizer` preprocessor along with `MultinomialNB` to perform sentiment analysis on the data set.  Address the following:\n",
    "   \n",
    "    a. How will you measure out-of-sample performance of the models?\n",
    "   \n",
    "    b. Compared to the baseline, does the model seem to have predictive power?\n",
    "   \n",
    "    c. Experiment with the following parameters of the model: `ngram_range`, `stop_words`, `binary`.\n",
    "   \n",
    "    d. Do any of the above parameters affect the model's performance?\n",
    "  \n",
    "4. (15 pts) Use the **spaCy** word embedding model, along with a supervised model of your choosing, to perform sentiment analysis.  Address the following:\n",
    "\n",
    "    a. How will you measure out-of-sample performance of the models?\n",
    "   \n",
    "    b. Compared to the baseline, does the model seem to have predictive power?\n",
    "\n",
    "    c. How does the word-embedding based model compare to the `MultinomialNB` model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "906d9d12-d35b-4f07-9bc1-4896477693cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy: 0.59\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"financial_phrasebank.csv\")\n",
    "\n",
    "# Baseline: predict the most frequent label\n",
    "most_frequent_label = data['label'].value_counts().idxmax()\n",
    "data['baseline_prediction'] = most_frequent_label\n",
    "\n",
    "# Calculate baseline accuracy\n",
    "baseline_accuracy = (data['label'] == data['baseline_prediction']).mean()\n",
    "print(f\"Baseline Accuracy: {baseline_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08eb5dac-0a6e-4007-8336-de6238acf618",
   "metadata": {},
   "source": [
    "### 1. Non-Machine Learning Baseline\n",
    "\n",
    "Baseline Accuracy: 0.59\n",
    "\n",
    "The majority-class baseline predicts the most frequent sentiment label (1 - Neutral) for all samples.\n",
    "\n",
    "A baseline accuracy of 0.59 indicates that this is the minimum standard for evaluating predictive models. A good model should outperform this baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7da712c4-6f01-4231-bd8f-1ab3606308c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.45      0.56       110\n",
      "           1       0.74      0.91      0.82       571\n",
      "           2       0.67      0.47      0.55       289\n",
      "\n",
      "    accuracy                           0.73       970\n",
      "   macro avg       0.72      0.61      0.64       970\n",
      "weighted avg       0.72      0.73      0.71       970\n",
      "\n",
      "ngram_range: (1, 1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.56      0.61       110\n",
      "           1       0.77      0.87      0.82       571\n",
      "           2       0.65      0.52      0.58       289\n",
      "\n",
      "    accuracy                           0.73       970\n",
      "   macro avg       0.69      0.65      0.67       970\n",
      "weighted avg       0.72      0.73      0.72       970\n",
      "\n",
      "ngram_range: (1, 2)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.45      0.56       110\n",
      "           1       0.74      0.91      0.82       571\n",
      "           2       0.66      0.47      0.55       289\n",
      "\n",
      "    accuracy                           0.73       970\n",
      "   macro avg       0.72      0.61      0.64       970\n",
      "weighted avg       0.72      0.73      0.71       970\n",
      "\n",
      "ngram_range: (2, 2)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.37      0.49       110\n",
      "           1       0.73      0.85      0.79       571\n",
      "           2       0.58      0.49      0.53       289\n",
      "\n",
      "    accuracy                           0.69       970\n",
      "   macro avg       0.67      0.57      0.60       970\n",
      "weighted avg       0.68      0.69      0.68       970\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['sentence'], data['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize CountVectorizer with parameters\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2), stop_words='english', binary=False)\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Train the MultinomialNB model\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test_vec)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Experiment with parameters\n",
    "for ngram_range in [(1, 1), (1, 2), (2, 2)]:\n",
    "    vectorizer = CountVectorizer(ngram_range=ngram_range, stop_words='english', binary=True)\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "    model.fit(X_train_vec, y_train)\n",
    "    y_pred = model.predict(X_test_vec)\n",
    "    print(f\"ngram_range: {ngram_range}\")\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3657a8-aa56-4fed-84e6-d4205fd47d76",
   "metadata": {},
   "source": [
    "### 2. CountVectorizer + MultinomialNB\n",
    "\n",
    "a. \n",
    "\n",
    "Out-of-sample performance is measured using a train-test split (80-20) to separate training and testing data. Evaluation metrics include accuracy, precision, recall, and F1-score, which provide insights into the model's ability to generalize to unseen data.\n",
    "\n",
    "b. \n",
    "\n",
    "Yes, the model achieves an accuracy of 0.73, significantly outperforming the baseline accuracy of 0.59. This indicates that the CountVectorizer + MultinomialNB model has predictive power.\n",
    "\n",
    "c.\n",
    "\n",
    "ngram_range:\n",
    "(1, 1) (unigrams) gives an accuracy of 0.73.\n",
    "(1, 2) (unigrams + bigrams) also achieves 0.73 but improves precision for class 0.\n",
    "(2, 2) (bigrams) reduces accuracy to 0.69.\n",
    "stop_words:\n",
    "Removing stopwords improves the model’s performance slightly by reducing noise in the feature set.\n",
    "binary:\n",
    "Setting binary=True (considering presence/absence instead of frequency) did not significantly impact performance.\n",
    "\n",
    "d. \n",
    "\n",
    "Yes, parameters like ngram_range and stop_words influence the model’s performance. Expanding ngram_range to include bigrams improves precision for certain classes but may overfit, leading to reduced overall accuracy. Removing stopwords reduces noise and enhances performance slightly. The binary parameter had minimal impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "34c6439d-3cbe-4317-ae1a-85260a0b517f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.06      0.12       110\n",
      "           1       0.67      0.99      0.80       571\n",
      "           2       0.70      0.28      0.40       289\n",
      "\n",
      "    accuracy                           0.68       970\n",
      "   macro avg       0.79      0.45      0.44       970\n",
      "weighted avg       0.72      0.68      0.60       970\n",
      "\n",
      "ngram_range: (1, 1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.05      0.10       110\n",
      "           1       0.68      0.98      0.80       571\n",
      "           2       0.67      0.34      0.45       289\n",
      "\n",
      "    accuracy                           0.68       970\n",
      "   macro avg       0.78      0.46      0.45       970\n",
      "weighted avg       0.71      0.68      0.62       970\n",
      "\n",
      "ngram_range: (1, 2)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.08      0.15       110\n",
      "           1       0.67      0.99      0.80       571\n",
      "           2       0.73      0.29      0.42       289\n",
      "\n",
      "    accuracy                           0.68       970\n",
      "   macro avg       0.80      0.45      0.46       970\n",
      "weighted avg       0.72      0.68      0.61       970\n",
      "\n",
      "ngram_range: (2, 2)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.09      0.17       110\n",
      "           1       0.67      0.99      0.80       571\n",
      "           2       0.66      0.27      0.38       289\n",
      "\n",
      "    accuracy                           0.67       970\n",
      "   macro avg       0.78      0.45      0.45       970\n",
      "weighted avg       0.71      0.67      0.60       970\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TfidfVectorizer with parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words='english', binary=False)\n",
    "X_train_vec = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_vec = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Train the MultinomialNB model\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test_vec)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Experiment with parameters\n",
    "for ngram_range in [(1, 1), (1, 2), (2, 2)]:\n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=ngram_range, stop_words='english', binary=True)\n",
    "    X_train_vec = tfidf_vectorizer.fit_transform(X_train)\n",
    "    X_test_vec = tfidf_vectorizer.transform(X_test)\n",
    "    model.fit(X_train_vec, y_train)\n",
    "    y_pred = model.predict(X_test_vec)\n",
    "    print(f\"ngram_range: {ngram_range}\")\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5962fd-7fa2-4b7f-bba0-1999573dc253",
   "metadata": {},
   "source": [
    "### 3. TfidVectorizer + MultinomialNB\n",
    "\n",
    "a. \n",
    "\n",
    "Similar to CountVectorizer, out-of-sample performance is measured using a train-test split (80-20). Metrics such as accuracy, precision, recall, and F1-score are used for evaluation.\n",
    "\n",
    "b. \n",
    "\n",
    "Yes, the TfidfVectorizer model achieves an accuracy of 0.68, which is better than the baseline accuracy of 0.59, demonstrating predictive power. However, it underperforms compared to CountVectorizer.\n",
    "\n",
    "c. \n",
    "\n",
    "ngram_range:\n",
    "(1, 1) achieves an accuracy of 0.68.\n",
    "(1, 2) maintains accuracy at 0.68 but slightly improves class 2 precision.\n",
    "(2, 2) reduces accuracy to 0.67.\n",
    "stop_words:\n",
    "Removing stopwords improves results by reducing irrelevant features.\n",
    "binary:\n",
    "Setting binary=True (presence/absence) had little impact on results.\n",
    "\n",
    "d. \n",
    "\n",
    "Yes, like with CountVectorizer, ngram_range and stop_words impact performance. Including bigrams slightly improves class precision but does not significantly enhance overall accuracy. Removing stopwords improves performance by reducing irrelevant features. The binary parameter has minimal effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a2c8c31f-3533-4df2-bd35-2769ec04c9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuanhanlim/anaconda3/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [22:37:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.64\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Convert sentences to embeddings\n",
    "def embed_sentence(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    return np.mean([token.vector for token in doc if token.has_vector], axis=0)\n",
    "\n",
    "# Generate embeddings\n",
    "X_train_embedded = np.array([embed_sentence(sent) for sent in X_train])\n",
    "X_test_embedded = np.array([embed_sentence(sent) for sent in X_test])\n",
    "\n",
    "# Train a supervised model (XGBoost)\n",
    "xgb_model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric=\"logloss\")\n",
    "xgb_model.fit(X_train_embedded, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = xgb_model.predict(X_test_embedded)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a12de4-0dd4-48c7-8a7a-33dcae457fdc",
   "metadata": {},
   "source": [
    "### 4. spaCy Word Embedding + XGBoost\n",
    "\n",
    "a. \n",
    "\n",
    "Performance is measured using a train-test split (80-20). Metrics like accuracy, precision, recall, and F1-score are used to evaluate the XGBoost classifier's ability to generalize.\n",
    "\n",
    "b. \n",
    "\n",
    "Yes, the word embedding-based XGBoost model achieves an accuracy of 0.64, outperforming the baseline accuracy of 0.59. However, it underperforms compared to the CountVectorizer model.\n",
    "\n",
    "c.\n",
    "\n",
    "The embedding-based model (accuracy 0.64) performs worse than CountVectorizer + MultinomialNB (0.73) and is comparable to TfidfVectorizer + MultinomialNB (0.68). While embeddings capture semantic relationships, they may lack context-sensitive information needed for this dataset and may require a larger dataset for better performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
